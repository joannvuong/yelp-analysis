{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Data Collection</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp data retrieval\n",
    "In this notebook, we will be using Python, Yelp API, and a third party API (scraperdog), to scrape data from yelp.  All of the data scraped is then saved into a CSV file to analyze later on. Our scraper will scrape each restaurant for 9 pages and grab 10 reviews per page. \n",
    "\n",
    "**NOTE:** API keys have been removed for privacy purposes, in order to run this, you will need to have your own key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from yelpapi import YelpAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "client_id = \" \"\n",
    "api_key = \" \"\n",
    "\n",
    "yelp_api = YelpAPI(api_key)\n",
    "\n",
    "#scraping restaurants from salt lake city, UT\n",
    "term = 'Restaurant'\n",
    "location = 'Salt Lake City, UT'\n",
    "search_limit = 50\n",
    "response = yelp_api.search_query(term = term,\n",
    "                                 location = location,\n",
    "                                 limit = search_limit)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapingdog_API_key = ' '\n",
    "yelp_url = 'https://www.yelp.com/biz/hallpass-salt-lake-city'\n",
    "page_limit = 9\n",
    "\n",
    "#Finding the page url and pages that we want to scrape for each restaurant\n",
    "def scrape_page(url, page, data_list):\n",
    "    scrape_url = url + '?start=' + str(page*10)\n",
    "    scrapingdog_url = 'https://api.scrapingdog.com/scrape?api_key=' + scrapingdog_API_key + '&url='\n",
    "    combine_url = scrapingdog_url + scrape_url + '&dynamic=True'\n",
    "    r = requests.get(combine_url).text\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    list_class = 'review__373c0__13kpL'\n",
    "    review_list = soup.find_all('div', {'class' : list_class})\n",
    "    print('scraping :', combine_url)\n",
    "    #loop over review list and adding to data_list\n",
    "    for l in review_list:\n",
    "        data = parse_review(l)\n",
    "        data_list.append(data)\n",
    "    #if the page is less than 10 and the length of the page is a full page then continue to the next page\n",
    "    if page < page_limit and len(review_list) == 10:\n",
    "        scrape_page(url, page+1, data_list)\n",
    "    else:\n",
    "        return data_list\n",
    "\n",
    "#Finding data that we might want for later, but mostly the reviews and reviewer ratings\n",
    "def parse_review(review):\n",
    "    content = review.find('p', {'class' : 'comment__373c0__1M-px css-n6i4z7'}).text\n",
    "    reviewer_rating = review.find('div', {'class' : 'i-stars__373c0__1T6rz'}).attrs['aria-label'].split()[0]\n",
    "    reviewer_date = review.find('span', {'class' : 'css-e81eai'}).text\n",
    "    reviewers_name = review.find('span',{'class':'css-m6anxm'}).text\n",
    "    reviewer_tags = review.find_all('button', {'class' : 'button__373c0__2Q-29'})\n",
    "    useful_tag = reviewer_tags[0].text[6:].strip()\n",
    "    if not useful_tag:\n",
    "        useful_tag = 0\n",
    "        \n",
    "    funny_tag = reviewer_tags[1].text[5:].strip()\n",
    "    if not funny_tag:\n",
    "        funny_tag = 0\n",
    "    \n",
    "    cool_tag = reviewer_tags[2].text[4:].strip()\n",
    "    if not cool_tag:\n",
    "        cool_tag = 0\n",
    "    \n",
    "    data = {'content' : content, 'review_rating' : reviewer_rating, 'date' : reviewer_date, 'reviewers_name' : reviewers_name, \n",
    "           'cool_tag' : cool_tag, 'useful_tag' : useful_tag, 'funny_tag' : funny_tag}    \n",
    "    return data\n",
    "\n",
    "#putting it all together into a CSV to analyze\n",
    "def scrape_business(business, is_first):\n",
    "    print('scraping business: ' + business['name'])\n",
    "    data_list = []\n",
    "    business_keys = ['name','categories','rating','coordinates','transactions','price', 'is_closed']\n",
    "    scrape_page(business['url'], 0, data_list)\n",
    "\n",
    "    #creating a csv of our reviews\n",
    "    with open('reviews.csv', 'a', encoding=\"utf8\", newline='') as file: \n",
    "        review_keys = ['content', 'review_rating', 'date', 'reviewers_name', 'cool_tag','useful_tag','funny_tag']\n",
    "        \n",
    "        #combining headers for both sets of data\n",
    "        dict_writer = csv.DictWriter(file, business_keys + review_keys)\n",
    "        \n",
    "        #writing out each individual CSV row\n",
    "        #creating the header for the first business only\n",
    "        if is_first:\n",
    "            dict_writer.writeheader()\n",
    "        for row in data_list:\n",
    "            for key in business_keys:\n",
    "                row[key] = business[key] if key in business else \"\"\n",
    "            dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #finding business info and calling function\n",
    "for index in range(len(response['businesses'])):\n",
    "    scrape_business(response['businesses'][index], index==0) #whether or not this is the first restaurant in the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c3c6bb65339d7520d2e7438dcdd1a229ce2672661888f6f2084471923c3363d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
